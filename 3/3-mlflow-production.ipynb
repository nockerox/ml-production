{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a7f4dc2",
   "metadata": {},
   "source": [
    "# –ß–∞—Å—Ç—å 3: –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b90ec4",
   "metadata": {},
   "source": [
    "## 1. MLFlow server tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08170f0c",
   "metadata": {},
   "source": [
    "mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root ./artifacts --host 0.0.0.0 --port 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6cfc46",
   "metadata": {},
   "source": [
    "## 2. –ü–æ–ª–Ω—ã–π –∂–∏–∑–Ω–µ–Ω–Ω—ã–π —Ü–∏–∫–ª –º–æ–¥–µ–ª–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "06810e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "import uuid\n",
    "import time\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51c55286",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Experiment: artifact_location='/home/nockerox/Dev/bigdata/lab4/1/artifacts/2', creation_time=1762958352550, experiment_id='2', last_update_time=1762958352550, lifecycle_stage='active', name='Taxi Demand Prediction', tags={'mlflow.experimentKind': 'custom_model_development'}>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "\n",
    "EXPERIMENT_NAME = \"Taxi Demand Prediction\"\n",
    "mlflow.set_experiment(EXPERIMENT_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f29b8c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = FeatureStore(repo_path=\"../2/nyc_taxi_demand/feature_repo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "94443940",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PULocationID</th>\n",
       "      <th>pickup_hour</th>\n",
       "      <th>trip_count</th>\n",
       "      <th>hour</th>\n",
       "      <th>dayofweek</th>\n",
       "      <th>lag_1h</th>\n",
       "      <th>lag_24h</th>\n",
       "      <th>lag_168h</th>\n",
       "      <th>rolling_mean_24h</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01 06:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01 07:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01 08:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2019-02-01 09:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PULocationID         pickup_hour  trip_count  hour  dayofweek  lag_1h  \\\n",
       "0             1 2019-02-01 01:00:00           1     1          4     0.0   \n",
       "1             1 2019-02-01 06:00:00           2     6          4     1.0   \n",
       "2             1 2019-02-01 07:00:00           1     7          4     2.0   \n",
       "3             1 2019-02-01 08:00:00           3     8          4     1.0   \n",
       "4             1 2019-02-01 09:00:00           1     9          4     3.0   \n",
       "\n",
       "   lag_24h  lag_168h  rolling_mean_24h  \n",
       "0      0.0       0.0               0.0  \n",
       "1      0.0       0.0               0.0  \n",
       "2      0.0       0.0               0.0  \n",
       "3      0.0       0.0               0.0  \n",
       "4      0.0       0.0               0.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_features = pd.read_parquet('../2/nyc_taxi_demand/feature_repo/data/demand_agg_with_ts.parquet')\n",
    "df_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f943677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: 2019-02-06 14:11:59.999999\n",
      "–ù–∞—á–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: 2019-02-01 00:00:00, –∫–æ–Ω–µ—Ü –¥–∞–Ω–Ω—ã—Ö: 2019-02-28 23:00:00\n"
     ]
    }
   ],
   "source": [
    "min_date = df_features['pickup_hour'].min()\n",
    "max_date = df_features['pickup_hour'].max()\n",
    "date_definition = min_date + (max_date - min_date) * (1 - 0.8)\n",
    "print(f\"–î–∞—Ç–∞ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è: {date_definition}\")\n",
    "print(f\"–ù–∞—á–∞–ª–æ –¥–∞–Ω–Ω—ã—Ö: {min_date}, –∫–æ–Ω–µ—Ü –¥–∞–Ω–Ω—ã—Ö: {max_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a11f43c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞ —Å–µ—Ä–µ–¥–∏–Ω—ã: 2019-02-03 19:05:59.999999\n"
     ]
    }
   ],
   "source": [
    "date_middle = min_date + (date_definition - min_date) * (1 - 0.5)\n",
    "print(f\"–î–∞—Ç–∞ —Å–µ—Ä–µ–¥–∏–Ω—ã: {date_middle}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9d7d0a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: 2019-02-02 22:58:11.999999\n"
     ]
    }
   ],
   "source": [
    "date_for_train = min_date + (date_definition - min_date) * (1 - 0.5) * 0.7\n",
    "print(f\"–î–∞—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è: {date_for_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "490eadf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ –¥–ª—è 11869 —Å–æ–±—ã—Ç–∏–π...\n",
      "Using pickup_hour as the event timestamp. To specify a column explicitly, please name it event_timestamp.\n",
      "–û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Feast:\n",
      "                pickup_hour  PULocationID  trip_count  lag_1h  lag_24h  \\\n",
      "0 2019-02-01 00:00:00+00:00           246         163     0.0      0.0   \n",
      "1 2019-02-01 00:00:00+00:00            14          71     0.0      0.0   \n",
      "2 2019-02-01 00:00:00+00:00            37         361     0.0      0.0   \n",
      "3 2019-02-01 00:00:00+00:00            86          18     0.0      0.0   \n",
      "4 2019-02-01 00:00:00+00:00           208          31     0.0      0.0   \n",
      "\n",
      "   lag_168h  rolling_mean_24h  hour  dayofweek  \n",
      "0       0.0               0.0     0          4  \n",
      "1       0.0               0.0     0          4  \n",
      "2       0.0               0.0     0          4  \n",
      "3       0.0               0.0     0          4  \n",
      "4       0.0               0.0     0          4  \n"
     ]
    }
   ],
   "source": [
    "train_entity_df = df_features[df_features['pickup_hour'] <= date_for_train].copy()\n",
    "train_entity_df = train_entity_df[['pickup_hour', 'PULocationID']]\n",
    "train_entity_df['PULocationID'] = train_entity_df['PULocationID'].astype('int64')\n",
    "\n",
    "print(f\"\\n–ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ –¥–ª—è {len(train_entity_df)} —Å–æ–±—ã—Ç–∏–π...\")\n",
    "\n",
    "features_to_get = [\n",
    "    \"taxi_stats:trip_count\",\n",
    "    \"taxi_stats:lag_1h\",\n",
    "    \"taxi_stats:lag_24h\",\n",
    "    \"taxi_stats:lag_168h\",\n",
    "    \"taxi_stats:rolling_mean_24h\",\n",
    "    \"taxi_stats:hour\",\n",
    "    \"taxi_stats:dayofweek\",\n",
    "]\n",
    "\n",
    "train_df = store.get_historical_features(\n",
    "    entity_df=train_entity_df,\n",
    "    features=features_to_get,\n",
    ").to_df()\n",
    "\n",
    "print(\"–û–±—É—á–∞—é—â–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Feast:\")\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a7c5b174",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "–ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ –¥–ª—è 5299 —Å–æ–±—ã—Ç–∏–π...\n",
      "Using pickup_hour as the event timestamp. To specify a column explicitly, please name it event_timestamp.\n",
      "–¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Feast:\n",
      "                pickup_hour  PULocationID  trip_count  lag_1h  lag_24h  \\\n",
      "0 2019-02-02 23:00:00+00:00            76         379   361.0    297.0   \n",
      "1 2019-02-02 23:00:00+00:00             8           1     2.0      0.0   \n",
      "2 2019-02-02 23:00:00+00:00           258         117   121.0     85.0   \n",
      "3 2019-02-02 23:00:00+00:00           226         279   215.0    226.0   \n",
      "4 2019-02-02 23:00:00+00:00           219          56    57.0     30.0   \n",
      "\n",
      "   lag_168h  rolling_mean_24h  hour  dayofweek  \n",
      "0       0.0        277.000000    23          5  \n",
      "1       0.0          0.000000    23          5  \n",
      "2       0.0         91.250000    23          5  \n",
      "3       0.0        209.000000    23          5  \n",
      "4       0.0         39.583333    23          5  \n"
     ]
    }
   ],
   "source": [
    "test_entity_df = df_features[(df_features['pickup_hour'] > date_for_train) & (df_features['pickup_hour'] <= date_middle)].copy()\n",
    "test_entity_df = test_entity_df[['pickup_hour', 'PULocationID']]\n",
    "test_entity_df['PULocationID'] = test_entity_df['PULocationID'].astype('int64')\n",
    "print(f\"\\n–ü–æ–ª—É—á–∞–µ–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Ñ–∏—á–∏ –¥–ª—è {len(test_entity_df)} —Å–æ–±—ã—Ç–∏–π...\")\n",
    "test_df = store.get_historical_features(\n",
    "    entity_df=test_entity_df,\n",
    "    features=features_to_get,\n",
    ").to_df()\n",
    "print(\"–¢–µ—Å—Ç–æ–≤—ã–π –¥–∞—Ç–∞—Å–µ—Ç –∏–∑ Feast:\")\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7814040d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- –û–±—É—á–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ ---\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000309 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 792\n",
      "[LightGBM] [Info] Number of data points in the train set: 11869, number of used features: 5\n",
      "[LightGBM] [Info] Start training from score 140.817508\n",
      "–ì–æ—Ç–æ–≤–æ.\n",
      "\n",
      "--- –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ ---\n",
      "MAE: 21.36\n",
      "RMSE: 47.34\n",
      "R2 Score: 0.9047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nockerox/Dev/bigdata/.venv/lib/python3.13/site-packages/mlflow/types/utils.py:452: UserWarning: Hint: Inferred schema contains integer column(s). Integer columns in Python cannot represent missing values. If your input data contains missing values at inference time, it will be encoded as floats and will cause a schema enforcement error. The best way to avoid this problem is to infer the model schema based on a realistic data sample (training dataset) that includes missing values. Alternatively, you can declare integer columns as doubles (float64) whenever these columns may have missing values. See `Handling Integers With Missing Values <https://www.mlflow.org/docs/latest/models.html#handling-integers-with-missing-values>`_ for more details.\n",
      "  warnings.warn(\n",
      "2025/11/13 20:47:06 WARNING mlflow.utils.environment: Failed to resolve installed pip version. ``pip`` will be added to conda.yaml environment spec without a version specifier.\n",
      "Registered model 'LGBM-Demand-Forecaster' already exists. Creating a new version of this model...\n",
      "2025/11/13 20:47:06 INFO mlflow.store.model_registry.abstract_store: Waiting up to 300 seconds for model version to finish creation. Model name: LGBM-Demand-Forecaster, version 18\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèÉ View run LGBM_Pipeline_Baseline_v3 at: http://localhost:5000/#/experiments/2/runs/114a64530c344cfb85a5ad6a46c720b6\n",
      "üß™ View experiment at: http://localhost:5000/#/experiments/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created version '18' of model 'LGBM-Demand-Forecaster'.\n"
     ]
    }
   ],
   "source": [
    "lgbm_params = {\n",
    "    'random_state': 42,\n",
    "    'n_estimators': 100,\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31\n",
    "}\n",
    "\n",
    "\n",
    "with mlflow.start_run(run_name='LGBM_Pipeline_Baseline_v3'):\n",
    "    mlflow.log_params(lgbm_params)\n",
    "    regressor = lgb.LGBMRegressor(random_state=42)\n",
    "\n",
    "    model_pipeline = Pipeline([\n",
    "        ('regressor', regressor)\n",
    "    ])\n",
    "\n",
    "    model_pipeline.set_params(regressor__random_state=lgbm_params['random_state'],\n",
    "                           regressor__n_estimators=lgbm_params['n_estimators'],\n",
    "                           regressor__learning_rate=lgbm_params['learning_rate'],\n",
    "                           regressor__num_leaves=lgbm_params['num_leaves'])\n",
    "\n",
    "    print(\"--- –û–±—É—á–µ–Ω–∏–µ –ø–∞–π–ø–ª–∞–π–Ω–∞ ---\")\n",
    "\n",
    "    X_train = train_df.drop(columns=['pickup_hour', 'PULocationID', 'trip_count'])\n",
    "    y_train = train_df['trip_count']\n",
    "    X_test = test_df.drop(columns=['pickup_hour', 'PULocationID', 'trip_count'])\n",
    "    y_test = test_df['trip_count']\n",
    "\n",
    "    model_pipeline.fit(X_train, y_train)\n",
    "    print(\"–ì–æ—Ç–æ–≤–æ.\")\n",
    "\n",
    "    print(\"\\n--- –û—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏ ---\")\n",
    "    predictions = model_pipeline.predict(X_test)\n",
    "\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    print(f\"MAE: {mae:.2f}\")\n",
    "    print(f\"RMSE: {rmse:.2f}\")\n",
    "    print(f\"R2 Score: {r2:.4f}\")\n",
    "\n",
    "    mlflow.log_metrics({\"mae\": mae, \"rmse\": rmse, \"r2_score\": r2})\n",
    "\n",
    "    input_example = X_train.head(5)\n",
    "\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model_pipeline,\n",
    "        name=\"demand_forecasting_pipeline\",\n",
    "        registered_model_name=\"LGBM-Demand-Forecaster\",\n",
    "        input_example=input_example\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b00585",
   "metadata": {},
   "source": [
    "## 3-4. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4940313c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "runs:/114a64530c344cfb85a5ad6a46c720b6/LGBM-Demand-Forecaster\n"
     ]
    }
   ],
   "source": [
    "current_experiment = mlflow.get_experiment_by_name(EXPERIMENT_NAME)\n",
    "experiment_id = current_experiment.experiment_id\n",
    "\n",
    "df_runs = mlflow.search_runs([experiment_id], order_by=[\"metrics.r2_score DESC\"])\n",
    "best_run_id = df_runs.loc[0, 'run_id']\n",
    "\n",
    "model_uri = \"runs:/\" + best_run_id + \"/\" + \"LGBM-Demand-Forecaster\"\n",
    "print(model_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b6a9a7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_8573/1022956840.py:2: FutureWarning: ``mlflow.tracking.client.MlflowClient.transition_model_version_stage`` is deprecated since 2.9.0. Model registry stages will be removed in a future major release. To learn more about the deprecation of model registry stages, see our migration guide here: https://mlflow.org/docs/latest/model-registry.html#migrating-from-stages\n",
      "  client.transition_model_version_stage(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE: 21.36\n",
      "RMSE: 47.34\n",
      "R2 Score: 0.9047\n"
     ]
    }
   ],
   "source": [
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name='LGBM-Demand-Forecaster',\n",
    "    version=7,\n",
    "    stage='Production'\n",
    ")\n",
    "\n",
    "model = mlflow.pyfunc.load_model(\"models:/LGBM-Demand-Forecaster/Production\")\n",
    "\n",
    "X_test = test_df.drop(columns=['pickup_hour', 'PULocationID', 'trip_count'])\n",
    "y_test = test_df['trip_count']\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "mae = mean_absolute_error(y_test, predictions)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "r2 = r2_score(y_test, predictions)\n",
    "print(f\"MAE: {mae:.2f}\")\n",
    "print(f\"RMSE: {rmse:.2f}\")\n",
    "print(f\"R2 Score: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "67349897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_model(model, x_test, y_test):\n",
    "    predictions = model.predict(x_test)\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    if r2 < 0.85:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "validate_model(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aae14ab4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "–î–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\n"
     ]
    }
   ],
   "source": [
    "X_train = train_df.drop(columns=['pickup_hour', 'PULocationID', 'trip_count'])\n",
    "y_train = train_df['trip_count']\n",
    "\n",
    "X_test = test_df.drop(columns=['pickup_hour', 'PULocationID', 'trip_count'])\n",
    "y_test = test_df['trip_count']\n",
    "\n",
    "reference_data_full = pd.concat([X_train, y_train], axis=1)\n",
    "current_data_full = pd.concat([X_test, y_test], axis=1)\n",
    "\n",
    "os.makedirs('monitoring/data', exist_ok=True)\n",
    "\n",
    "reference_data_full.to_parquet('monitoring/data/reference_data.parquet')\n",
    "current_data_full.to_parquet('monitoring/data/current_data.parquet')\n",
    "\n",
    "print(\"–î–∞–Ω–Ω—ã–µ –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bigdata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
