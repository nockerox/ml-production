import os
import requests
import pandas as pd
import mlflow
from mlflow.tracking import MlflowClient
import datetime
import json
from evidently import Report
from evidently.presets import DataDriftPreset, RegressionPreset
from evidently.legacy.pipeline.column_mapping import ColumnMapping

# --- 1. –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
MLFLOW_TRACKING_URI = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)
EXPERIMENT_NAME = "Taxi Demand Prediction"
mlflow.set_experiment(EXPERIMENT_NAME)

SLACK_WEBHOOK_URL = os.getenv("SLACK_WEBHOOK_URL")

# –ü–æ—Ä–æ–≥–∏
MODEL_PERFORMANCE_DEGRADATION_MAE_THRESHOLD = 1.2 
RETRAIN_PERFORMANCE_DEGRADATION_MAE_THRESHOLD = 1.3

# --- 2. –§—É–Ω–∫—Ü–∏–∏ –¥–ª—è –∞–ª–µ—Ä—Ç–∏–Ω–≥–∞ ---
def send_alert(message: str, is_critical: bool = False):
    prefix = "üö® *–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –∞–ª–µ—Ä—Ç* üö®" if is_critical else "‚ö†Ô∏è *–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ* ‚ö†Ô∏è"
    full_message = f"{prefix}\n{message}"
    
    print(full_message)
    
    if not SLACK_WEBHOOK_URL:
        print("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è SLACK_WEBHOOK_URL –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞. –ê–ª–µ—Ä—Ç –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ Slack.")
        return
        
    try:
        payload = {"blocks": [{"type": "section", "text": {"type": "mrkdwn", "text": full_message}}]}
        requests.post(SLACK_WEBHOOK_URL, json=payload, timeout=5)
        print("–ê–ª–µ—Ä—Ç —É—Å–ø–µ—à–Ω–æ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω –≤ Slack.")
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ –∞–ª–µ—Ä—Ç–∞ –≤ Slack: {e}")

# --- 3. –§—É–Ω–∫—Ü–∏–∏ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ–¥ API —Å Snapshot) ---
def monitor_data_drift(reference_data: pd.DataFrame, current_data: pd.DataFrame) -> dict:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –¥—Ä–∏—Ñ—Ç–µ –¥–∞–Ω–Ω—ã—Ö, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—á–Ω—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É JSON."""
    print("\n--- –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –¥—Ä–∏—Ñ—Ç–∞ –¥–∞–Ω–Ω—ã—Ö ---")
    
    report = Report(metrics=[DataDriftPreset()])
    snapshot = report.run(reference_data=reference_data, current_data=current_data)
    
    report_dict = json.loads(snapshot.json())

    try:
        drift_metric_value = report_dict['metrics'][0]['value']
        num_drifted_columns = int(drift_metric_value['count'])
        dataset_drift_detected = num_drifted_columns > 0
        
    except (KeyError, IndexError, TypeError) as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥—Ä–∏—Ñ—Ç–∞ –∏–∑ –æ—Ç—á–µ—Ç–∞: {e}")
        return {"dataset_drift": False, "drifted_columns": 0}

    with mlflow.start_run(run_name="Data Drift Report"):
        snapshot.save_html("data_drift_report.html")
        mlflow.log_artifact("data_drift_report.html", "reports")
        mlflow.log_dict(report_dict, "data_drift_report.json")
        
        mlflow.log_metric("num_drifted_columns", num_drifted_columns)
        mlflow.log_metric("dataset_drift", int(dataset_drift_detected))

    print(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –¥—Ä–∏—Ñ—Ç –≤ {num_drifted_columns} –∫–æ–ª–æ–Ω–∫–∞—Ö.")
    if dataset_drift_detected:
        send_alert(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω –æ–±—â–∏–π –¥—Ä–∏—Ñ—Ç –¥–∞–Ω–Ω—ã—Ö! –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–º–µ—â–µ–Ω–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫: {num_drifted_columns}.")

    return {"dataset_drift": dataset_drift_detected, "drifted_columns": num_drifted_columns}


def monitor_model_performance(model, reference_data: pd.DataFrame, current_data: pd.DataFrame) -> dict:
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ (—Å —è–≤–Ω—ã–º —É–∫–∞–∑–∞–Ω–∏–µ–º –∑–∞–¥–∞—á–∏)."""
    print("\n--- –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏ ---")

    ref_data_copy = reference_data.copy()
    curr_data_copy = current_data.copy()

    ref_predictions = model.predict(ref_data_copy.drop('trip_count', axis=1))
    curr_predictions = model.predict(curr_data_copy.drop('trip_count', axis=1))
    
    ref_data_copy['target'] = ref_data_copy['trip_count']
    curr_data_copy['target'] = curr_data_copy['trip_count']
    ref_data_copy.drop('trip_count', axis=1, inplace=True)
    curr_data_copy.drop('trip_count', axis=1, inplace=True)

    ref_data_copy['prediction'] = ref_predictions
    curr_data_copy['prediction'] = curr_predictions
    
    from evidently.legacy.pipeline.column_mapping import TaskType

    column_mapping = ColumnMapping(
        target='target', 
        prediction='prediction',
        task=TaskType.REGRESSION_TASK
    )
    
    report = Report(metrics=[RegressionPreset()])
    
    try:
        snapshot = report.run(reference_data=ref_data_copy, 
                              current_data=curr_data_copy,
                              column_mapping=column_mapping)
    except TypeError as e:
         print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–µ—Ä–µ–¥–∞—Ç—å column_mapping –≤ run. –û—à–∏–±–∫–∞: {e}")
         return {"reference_mae": -1, "current_mae": -1}

    report_dict = json.loads(snapshot.json())

    try:
        quality_widget_results = report_dict['widgets'][1]['results']
        ref_mae = quality_widget_results['reference']['mean_abs_error']
        curr_mae = quality_widget_results['current']['mean_abs_error']
    except (KeyError, IndexError) as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ—Ç—Ä–∏–∫ –∫–∞—á–µ—Å—Ç–≤–∞ –∏–∑ –æ—Ç—á–µ—Ç–∞: {e}")
        print("--- –°–¢–†–£–ö–¢–£–†–ê JSON –û–¢–ß–ï–¢–ê –û –ö–ê–ß–ï–°–¢–í–ï ---")
        print(json.dumps(report_dict, indent=4))
        return {"reference_mae": -1, "current_mae": -1}

    with mlflow.start_run(run_name="Model Performance Report"):
        snapshot.save_html("model_performance_report.html")
        mlflow.log_artifact("model_performance_report.html", "reports")
        mlflow.log_dict(report_dict, "model_performance_report.json")
        mlflow.log_metrics({"reference_mae": ref_mae, "current_mae": curr_mae})

    print(f"Reference MAE: {ref_mae:.2f}, Current MAE: {curr_mae:.2f}")

    if ref_mae > 0 and curr_mae > ref_mae * MODEL_PERFORMANCE_DEGRADATION_MAE_THRESHOLD:
        degradation = ((curr_mae / ref_mae) - 1) * 100
        send_alert(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–∏! MAE —É–≤–µ–ª–∏—á–∏–ª—Å—è –Ω–∞ {degradation:.2f}% (—Å {ref_mae:.2f} –¥–æ {curr_mae:.2f}).")
        
    return {"reference_mae": ref_mae, "current_mae": curr_mae}

# --- 4. –õ–æ–≥–∏–∫–∞ —Ä–µ—Ç—Ä–µ–π–Ω–∞ ---
def retrain_model():
    send_alert("–ó–∞–ø—É—â–µ–Ω–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä–∞ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–≥–æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏.", is_critical=True)

def check_and_run_retrain(data_drift_info: dict, model_performance_info: dict):
    print("\n--- –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è ---")
    retrain_needed = False
    reason = ""

    if datetime.date.today().weekday() == 0:
        retrain_needed = True
        reason = "–ü–ª–∞–Ω–æ–≤–æ–µ –µ–∂–µ–Ω–µ–¥–µ–ª—å–Ω–æ–µ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ."

    if data_drift_info.get('dataset_drift', False) and not retrain_needed:
        retrain_needed = True
        reason = "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –¥—Ä–∏—Ñ—Ç –¥–∞–Ω–Ω—ã—Ö."

    ref_mae = model_performance_info.get('reference_mae', -1)
    curr_mae = model_performance_info.get('current_mae', -1)
    if ref_mae > 0 and curr_mae > ref_mae * RETRAIN_PERFORMANCE_DEGRADATION_MAE_THRESHOLD and not retrain_needed:
        retrain_needed = True
        reason = "–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –¥–µ–≥—Ä–∞–¥–∞—Ü–∏—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏."

    if retrain_needed:
        print(f"–ü—Ä–∏–Ω—è—Ç–æ —Ä–µ—à–µ–Ω–∏–µ –æ –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–∏. –ü—Ä–∏—á–∏–Ω–∞: {reason}")
        retrain_model()
    else:
        print("–ü–µ—Ä–µ–æ–±—É—á–µ–Ω–∏–µ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç—Å—è.")

# --- 5. –û—Å–Ω–æ–≤–Ω–æ–π –ø–∞–π–ø–ª–∞–π–Ω ---
if __name__ == "__main__":
    base_dir = os.path.dirname(os.path.abspath(__file__))
    ref_data_path = os.path.join(base_dir, '..', '3', 'monitoring', 'data', 'reference_data.parquet')
    curr_data_path = os.path.join(base_dir, '..', '3', 'monitoring', 'data', 'current_data.parquet')

    try:
        ref_data = pd.read_parquet(ref_data_path)
        curr_data = pd.read_parquet(curr_data_path)
    except FileNotFoundError:
        print(f"–û—à–∏–±–∫–∞: –§–∞–π–ª—ã –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –ø–æ –ø—É—Ç—è–º:\n{ref_data_path}\n{curr_data_path}")
        exit()

    client = MlflowClient()
    try:
        latest_versions = client.get_latest_versions("LGBM-Demand-Forecaster", stages=["Production"])
        if not latest_versions:
            raise IndexError("No model versions found in Production stage.")
        prod_model_info = latest_versions[0]
        model_uri = f"models:/{prod_model_info.name}/{prod_model_info.version}"
        production_model = mlflow.pyfunc.load_model(model_uri)
    except IndexError as e:
        print(f"–û—à–∏–±–∫–∞: {e}. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ –≤–µ—Ä—Å–∏—è –º–æ–¥–µ–ª–∏ –∏–º–µ–µ—Ç —Å—Ç–µ–π–¥–∂ 'Production'.")
        exit()
    except Exception as e:
        print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        exit()

    data_drift_results = monitor_data_drift(ref_data.drop('trip_count', axis=1), curr_data.drop('trip_count', axis=1))
    model_performance_results = monitor_model_performance(production_model, ref_data, curr_data)
    
    check_and_run_retrain(data_drift_results, model_performance_results)
